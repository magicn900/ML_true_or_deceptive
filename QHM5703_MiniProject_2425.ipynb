{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ QHM5703 MiniProject 2425 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaGn4ICrfqXZ"
   },
   "source": [
    "# 1 Author\n",
    "\n",
    "**Student Name**:  Feifan Liu\n",
    "**Student ID**:  221155682\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o38VQkcdKd6k"
   },
   "source": [
    "# 2 Problem formulation\n",
    "\n",
    "Describe the machine learning problem that you want to solve and explain what's interesting about it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "[ What ]\n",
    "This project addresses the challenging task of determining whether a narrated story is true or deceptive using machine learning techniques. \n",
    "Given a 30-second audio recording of someone telling a story, \n",
    "our goal is to build a model that can classify the narrative as either truthful or deceptive.\n",
    "\n",
    "\n",
    "[ Why ]\n",
    "The problem is particularly interesting as it combines elements of audio processing with deception detection\n",
    " - a field that traditionally relies on human expertise and judgment. \n",
    "By leveraging machine learning, we aim to identify subtle patterns in voice characteristics, speech rhythms, \n",
    "and other acoustic features that might indicate deception. \n",
    "These patterns could be imperceptible to the human ear but detectable through computational analysis.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPTSuaB9L2jU"
   },
   "source": [
    "# 3 Methodology\n",
    "\n",
    "Describe your methodology. Specifically, describe your training task and validation task, and how model performance is defined (i.e. accuracy, confusion matrix, etc). Any other tasks that might help you build your model should also be described here."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "[ Feature engineering ]\n",
    "By searching and browsing academic papers, select suitable features.\n",
    "\n",
    "[ Training task ]\n",
    "Try using multiple machine learning models to classify the data ,\n",
    "and use grid search to find the optimal parameters.\n",
    "\n",
    "[ Validation task ]\n",
    "Use a 5-fold cross-validation to evaluate the model's performance.\n",
    "\n",
    "[ Definition of model performance ]\n",
    "Simply use accuracy as the metric for evaluating the model's performance.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3BwrtEdLDit"
   },
   "source": [
    "# 4 Implemented ML prediction pipelines\n",
    "\n",
    "Describe the ML prediction pipelines that you will explore. Clearly identify their input and output, stages and format of the intermediate data structures moving from one stage to the next. It's up to you to decide which stages to include in your pipeline. After providing an overview, describe in more detail each one of the stages that you have included in their corresponding subsections (i.e. 4.1 Transformation stage, 4.2 Model stage, 4.3 Ensemble stage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1nDXnzYLLH6"
   },
   "source": [
    "## 4.1 Transformation stage\n",
    "\n",
    "Describe any transformations, such as feature extraction. Identify input and output. Explain why you have chosen this transformation stage."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "[ Input ]\n",
    "A piece of audio data (30-second recording of someone telling a story)\n",
    "\n",
    "\n",
    "\n",
    "[ Output ]\n",
    "In the feature extraction process, \n",
    "we used MFCC (Mel-frequency cepstral coefficients) and pitch as the main features. \n",
    "Specifically, we extracted 40 MFCC parameters and 1 pitch parameter, \n",
    "which can effectively capture the spectral features of the audio signal. \n",
    "\n",
    "\n",
    "We tried two types of features: \n",
    "1. Average Feature: \n",
    "Take the average of each MFCC parameter and pitch for each frame to obtain a fixed-length feature vector.\n",
    "\n",
    "2. Temporally Related Sequence Feature: \n",
    "Retain the features of all frames in the audio data to form a time-sequence feature vector.\n",
    "\n",
    "\n",
    "\n",
    "[ Why ]\n",
    "Through browsing the papers, \n",
    "I found that MFCC and pitch have good results in human audio parsing, \n",
    "so I directly selected these two as the features of this project.\n",
    "\n",
    "I found in (Thaler et al., 2021) that 40 MFCC features performed better than 13 MFCC features, \n",
    "so for convenience I directly chose 40 MFCC features.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F5_kI95LuZ2"
   },
   "source": [
    "## 4.2 Model stage\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "[ Models for Dataset A ]\n",
    "Dataset A: The dataset which contains the average features of audio.\n",
    "\n",
    "Description:\n",
    "I will experiment with several classic machine learning models including \n",
    "SVM, Logistic Regression, Decision Tree, Random Forest, KNeighbors and Gaussian Naive Bayes. \n",
    "Grid search will be employed for hyperparameter tuning, and 5-fold cross validation will be used to evaluate model accuracy. \n",
    "This comprehensive approach allows me to compare different algorithms \n",
    "and identify the most effective one for this audio classification task.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[ Models for Dataset B ]\n",
    "Dataset B: The dataset which contains the temporally related sequence features of audio.\n",
    "\n",
    "Description:\n",
    "This data set considers time series data, \n",
    "so I partially reproduce the compound model of CNN+LSTM in (Thaler et al., 2021) with AI tools, \n",
    "hoping to obtain better classification results by using both CNN's ability to capture local patterns and LSTM's strength in handling sequential data.\n",
    "\n",
    "The CNN-LSTM model architecture is structured as follows:\n",
    "\n",
    "    Input: [batch_size, timesteps, 41] features \n",
    "    ↓\n",
    "    CNN (Conv1D layers) → Extract temporal features\n",
    "    ↓ \n",
    "    LSTM layers → Process sequential patterns\n",
    "    ↓\n",
    "    Dense layers → Output classification probability\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Ensemble stage\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "[ Models for Dataset A ]\n",
    "In fact, the random forest model is an ensemble decision tree model, \n",
    "but since I directly called the pre-packaged methods in the sklearn library and skipped the implementation steps, \n",
    "I omitted it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[ Models for Dataset B ]\n",
    "I think CNN-LSTM model is also an ensemble model.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZQPxztuL9AW"
   },
   "source": [
    "# 5 Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "First, I split all the audio files into multiple 30s short audio segments \n",
    "and generated a new label file MLEndDD_story_attributes_30s.csv \n",
    "based on MLEndDD_story_attributes_small.csv.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, re, pickle, glob\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import spkit as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Split audio files into 30s segments ########\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "sample_path = './MLEndDD_stories_small/*.wav'\n",
    "files = glob.glob(sample_path)\n",
    "\n",
    "\n",
    "output_dir = './output_30s/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Split audio files into 30s segments\n",
    "for file in files:\n",
    "    audio = AudioSegment.from_wav(file)\n",
    "    duration = len(audio)\n",
    "    segment_duration = 30000  # 30s\n",
    "    num_segments = duration // segment_duration\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start_time = i * segment_duration\n",
    "        end_time = start_time + segment_duration\n",
    "        audio_segment = audio[start_time:end_time]\n",
    "        output_file = os.path.join(output_dir, f\"{os.path.basename(file).split('.')[0]}_segment_{i+1}.wav\")\n",
    "        with open(output_file, 'wb') as f:\n",
    "            audio_segment.export(f, format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 420 rows to MLEndDD_story_attributes_30s.csv\n"
     ]
    }
   ],
   "source": [
    "######## Create CSV file with story attributes for 30s segments ########\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "MLEND_df = pd.read_csv('./MLEndDD_story_attributes_small.csv')\n",
    "\n",
    "new_rows = []\n",
    "\n",
    "segment_path = './output_30s/*.wav'\n",
    "segment_files = glob.glob(segment_path)\n",
    "\n",
    "for segment_file in segment_files:\n",
    "    segment_filename = os.path.basename(segment_file)\n",
    "    original_filename = segment_filename.split('_segment_')[0] + '.wav'\n",
    "    \n",
    "    original_row = MLEND_df[MLEND_df['filename'] == original_filename].iloc[0]\n",
    "    \n",
    "    new_rows.append({\n",
    "        'filename': segment_filename,\n",
    "        'Language': original_row['Language'],\n",
    "        'Story_type': original_row['Story_type']\n",
    "    })\n",
    "\n",
    "new_MLEND_df = pd.DataFrame(new_rows)\n",
    "\n",
    "new_MLEND_df.to_csv('./MLEndDD_story_attributes_30s.csv', index=False)\n",
    "\n",
    "print(f\"Saved {len(new_MLEND_df)} rows to MLEndDD_story_attributes_30s.csv\")\n",
    "\n",
    "\n",
    "######## Store the path of the 30s segments in a pickle file ########\n",
    "files = 'output_30s/' + new_MLEND_df['filename']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Dataset A"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Dataset A: The dataset which contains the average features of audio.\n",
    "\n",
    "---\n",
    "\n",
    "Due to the small amount of data, \n",
    "I initially intended to include all segments of each audio file in the dataset. \n",
    "\n",
    "However, after doing this, \n",
    "I found that if I split different segments of the same audio file into the training and testing sets, \n",
    "it would lead to abnormally high scores for the model. \n",
    "\n",
    "I suspect this high performance is due to data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "To avoid this problem, I proposed two solutions. \n",
    "\n",
    "The first is to only take the first 30 seconds, which is the first segment of each audio file, \n",
    "and include it in the dataset, resulting in a smaller amount of data. \n",
    "\n",
    "The second is to first divide the training and testing sets based on the files, \n",
    "and then include the segments. \n",
    "\n",
    "Although the second method seems to be more thorough, it appears that due to overfitting or other reasons, \n",
    "the first method yields better training results for the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "After the processing is completed, X_a, which consists of vectors of dimension 41, \n",
    "and y_a, which consists of either 0 or 1, are obtained.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(audio_path, n_mfcc=13, frame_length=0.03, frame_stride=0.03, sr=16000):\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_path, sr=sr)\n",
    "    \n",
    "    # Compute MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, \n",
    "                                hop_length=int(frame_stride*sr), \n",
    "                                n_fft=int(frame_length*sr))\n",
    "    \n",
    "    # Compute pitch (音高) features\n",
    "    pitches, magnitudes = librosa.core.piptrack(y=audio, sr=sr, \n",
    "                                                hop_length=int(frame_stride*sr), \n",
    "                                                n_fft=int(frame_length*sr))\n",
    "    \n",
    "    # Select the highest pitch for each frame\n",
    "    pitch = []\n",
    "    for t in range(pitches.shape[1]):\n",
    "        index = magnitudes[:, t].argmax()\n",
    "        pitch.append(pitches[index, t])\n",
    "    pitch = np.array(pitch)\n",
    "    \n",
    "    # Combine MFCC and pitch features\n",
    "    features = np.concatenate((mfcc, pitch[np.newaxis, :]), axis=0)\n",
    "    \n",
    "    # Compute the mean of each feature\n",
    "    features_mean = np.mean(features, axis=1)\n",
    "    \n",
    "    return features_mean\n",
    "\n",
    "def process_audio_files(dataframe, audio_dir, n_mfcc=13, language=None, first_segment_only=False):\n",
    "    \"\"\"\n",
    "    处理指定语言的音频文件，并根据 dataframe 中的标签提取特征。\n",
    "    Args:\n",
    "        dataframe: 包含文件名、语言和故事类型的 DataFrame。\n",
    "        audio_dir: 存放音频文件的目录。\n",
    "        n_mfcc: 每帧提取的 MFCC 特征数量。\n",
    "        language: 需要筛选的语言（如 'English'），如果为 None，则处理所有语言。\n",
    "        first_segment_only: 是否只处理每个音频文件的第一个片段。\n",
    "    Returns:\n",
    "        features: 提取的 MFCC 特征数组。\n",
    "        labels: 对应的标签数组。\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # 筛选指定语言的数据\n",
    "    if language:\n",
    "        dataframe = dataframe[dataframe['Language'] == language]\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        filename = row['filename']\n",
    "        label = row['Story_type']  # 假设 Story_type 是你的标签列\n",
    "        file_path = os.path.join(audio_dir, filename)\n",
    "        \n",
    "        if first_segment_only and not filename.endswith('_segment_1.wav'):\n",
    "            continue\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                mfcc_features = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "                features.append(mfcc_features)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.str_('deceptive_story'): np.int64(0), np.str_('true_story'): np.int64(1)}\n",
      "Training set shape: (54, 41), (54,)\n",
      "Validation set shape: (24, 41), (24,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" # Use the original filename as the group\\nnew_MLEND_df['file_prefix'] = new_MLEND_df['filename'].apply(lambda x: x.split('_segment_')[0])\\n\\n# Split the data into training and validation sets\\ngss = GroupShuffleSplit(n_splits=1, test_size=0.3)\\ntrain_idx, val_idx = next(gss.split(new_MLEND_df, groups=new_MLEND_df['file_prefix']))\\n\\ntrain_df = new_MLEND_df.iloc[train_idx]\\nval_df = new_MLEND_df.iloc[val_idx]\\n\\n# Extract MFCC features for each 30s segment\\nX_train, y_train = process_audio_files(train_df, audio_dir='output_30s/', n_mfcc=40, language='English', first_segment_only=False)\\nX_val, y_val = process_audio_files(val_df, audio_dir='output_30s/', n_mfcc=40, language='English', first_segment_only=False)\\n\\n# Transform the labels to integers\\nlabel_encoder = LabelEncoder()\\ny_train = label_encoder.fit_transform(y_train)\\ny_val = label_encoder.transform(y_val)\\n\\nprint(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\\n\\n# Normalize the features\\nmean = X_train.mean(0)\\nsd = X_train.std(0)\\n\\nX_train = (X_train - mean) / sd\\nX_val = (X_val - mean) / sd\\n\\nprint(f'Training set shape: {X_train.shape}, {y_train.shape}')\\nprint(f'Validation set shape: {X_val.shape}, {y_val.shape}') \""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract MFCC features for each 30s segment\n",
    "\n",
    "## Only Choose the first segment of each story\n",
    "X_a, y_a = process_audio_files(new_MLEND_df, audio_dir='output_30s/', n_mfcc=40, language='English', first_segment_only=True)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# transform the labels to integers\n",
    "y_a = label_encoder.fit_transform(y_a)\n",
    "\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "\n",
    "# Normalize the features\n",
    "mean = X_a.mean(0)\n",
    "sd = X_a.std(0)\n",
    "\n",
    "X_a = (X_a - mean) / sd\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_a, y_a, test_size=0.3)\n",
    "\n",
    "print(f'Training set shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Validation set shape: {X_val.shape}, {y_val.shape}')\n",
    "\n",
    "\n",
    "\n",
    "## Group audio segments by original story and split into training and validation sets\n",
    "\"\"\" # Use the original filename as the group\n",
    "new_MLEND_df['file_prefix'] = new_MLEND_df['filename'].apply(lambda x: x.split('_segment_')[0])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.3)\n",
    "train_idx, val_idx = next(gss.split(new_MLEND_df, groups=new_MLEND_df['file_prefix']))\n",
    "\n",
    "train_df = new_MLEND_df.iloc[train_idx]\n",
    "val_df = new_MLEND_df.iloc[val_idx]\n",
    "\n",
    "# Extract MFCC features for each 30s segment\n",
    "X_train, y_train = process_audio_files(train_df, audio_dir='output_30s/', n_mfcc=40, language='English', first_segment_only=False)\n",
    "X_val, y_val = process_audio_files(val_df, audio_dir='output_30s/', n_mfcc=40, language='English', first_segment_only=False)\n",
    "\n",
    "# Transform the labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "\n",
    "print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "# Normalize the features\n",
    "mean = X_train.mean(0)\n",
    "sd = X_train.std(0)\n",
    "\n",
    "X_train = (X_train - mean) / sd\n",
    "X_val = (X_val - mean) / sd\n",
    "\n",
    "print(f'Training set shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Validation set shape: {X_val.shape}, {y_val.shape}') \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Dataset B"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "[ Models for Dataset B ]\n",
    "\n",
    "Dataset B: The dataset which contains the temporally related sequence features of audio.\n",
    "\n",
    "---\n",
    "\n",
    "Data set B is processed in A similar way to data set A, \n",
    "except that the features extracted from all frames are changed from averaging to retaining all.\n",
    "\n",
    "---\n",
    "\n",
    "After the processing is completed, X_b, which consists of vector sequences of dimension 41,\n",
    "and y_b, which consists of either 0 or 1, are obtained.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_sequence(audio_path, n_mfcc=13, frame_length=0.186, frame_stride=0.186, sr=16000):\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_path, sr=sr)\n",
    "    \n",
    "    # Compute MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, \n",
    "                                hop_length=int(frame_stride*sr), \n",
    "                                n_fft=int(frame_length*sr))\n",
    "    \n",
    "    # Compute pitch features\n",
    "    pitches, magnitudes = librosa.core.piptrack(y=audio, sr=sr, \n",
    "                                                hop_length=int(frame_stride*sr), \n",
    "                                                n_fft=int(frame_length*sr))\n",
    "    \n",
    "    # Select the highest pitch for each frame\n",
    "    pitch = []\n",
    "    for t in range(pitches.shape[1]):\n",
    "        index = magnitudes[:, t].argmax()\n",
    "        pitch.append(pitches[index, t])\n",
    "    pitch = np.array(pitch)\n",
    "    \n",
    "    # Combine MFCC and pitch features\n",
    "    features = np.concatenate((mfcc.T, pitch[:, np.newaxis]), axis=1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def process_audio_files_sequence(dataframe, audio_dir, n_mfcc=13, language=None, first_segment_only=False):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Filters data for the specified language\n",
    "    if language:\n",
    "        dataframe = dataframe[dataframe['Language'] == language]\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        filename = row['filename']\n",
    "        label = row['Story_type']\n",
    "        file_path = os.path.join(audio_dir, filename)\n",
    "        \n",
    "        if first_segment_only and not filename.endswith('_segment_1.wav'):\n",
    "            continue\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                feature = extract_features_sequence(file_path, n_mfcc=n_mfcc)\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only Choose the first segment of each story\n",
    "X_b, y_b = process_audio_files_sequence(new_MLEND_df, audio_dir='output_30s/', n_mfcc=40, language='English', first_segment_only=True)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# transform the labels to integers\n",
    "y_b = label_encoder.fit_transform(y_b)\n",
    "\n",
    "\n",
    "# Normalize the features\n",
    "mean = X_b.mean(0)\n",
    "sd = X_b.std(0)\n",
    "\n",
    "X_b = (X_b - mean) / sd\n",
    "\n",
    "\n",
    "\n",
    "## Group audio segments by original story and split into training and validation sets\n",
    "# Partition the dataset by file prefix grouping\n",
    "\"\"\" gss = GroupShuffleSplit(n_splits=1, test_size=0.3)\n",
    "train_idx_b, val_idx_b = next(gss.split(new_MLEND_df, groups=new_MLEND_df['file_prefix']))\n",
    "\n",
    "train_df_b = new_MLEND_df.iloc[train_idx_b]\n",
    "val_df_b = new_MLEND_df.iloc[val_idx_b]\n",
    "\n",
    "X_train_b, y_train_b = process_audio_files_sequence(train_df_b, audio_dir='output_30s/', n_mfcc=40, language='English', first_segment_only=False)\n",
    "X_val_b, y_val_b = process_audio_files_sequence(val_df_b, audio_dir='output_30s/', n_mfcc=40, language='English', first_segment_only=False)\n",
    "\n",
    "label_encoder_b = LabelEncoder()\n",
    "y_train_b = label_encoder_b.fit_transform(y_train_b)\n",
    "y_val_b = label_encoder_b.transform(y_val_b)\n",
    "\n",
    "print(dict(zip(label_encoder_b.classes_, label_encoder_b.transform(label_encoder_b.classes_))))\n",
    "\n",
    "# Normalize the features\n",
    "mean_b = X_train_b.mean(0)\n",
    "sd_b = X_train_b.std(0)\n",
    "\n",
    "X_train_b = (X_train_b - mean_b) / sd_b\n",
    "X_val_b = (X_val_b - mean_b) / sd_b\n",
    "\n",
    "print(f'Training set shape: {X_train_b.shape}, {y_train_b.shape}')\n",
    "print(f'Validation set shape: {X_val_b.shape}, {y_val_b.shape}') \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qf7GN1aeXJI"
   },
   "source": [
    "# 6 Experiments and results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Dataset A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Best Cross-Validation Accuracy: 0.5416666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "model = svm.SVC()\n",
    "\n",
    "# define the grid search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_a, y_a)\n",
    "\n",
    "\n",
    "print('Best Parameters:', grid_search.best_params_)\n",
    "print('Best Cross-Validation Accuracy:', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "d:\\资料\\QM\\ML with Py\\QHM5703_MiniProject\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters of Logistic Regression: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best Cross-Validation Accuracy of Logistic Regression: 0.5541666666666667\n",
      "Best Parameters of Decision Tree: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 5}\n",
      "Best Cross-Validation Accuracy of Decision Tree: 0.5658333333333333\n",
      "Best Parameters of Random Forest: {'bootstrap': False, 'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 10}\n",
      "Best Cross-Validation Accuracy of Random Forest: 0.6033333333333333\n",
      "Best Parameters of KNN: {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'uniform'}\n",
      "Best Cross-Validation Accuracy of KNN: 0.5391666666666667\n",
      "Best Parameters of Naive Bayes: {}\n",
      "Best Cross-Validation Accuracy of Naive Bayes: 0.4766666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# define the parameter grid for each model\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "param_grid_nb = {}\n",
    "\n",
    "# define the models\n",
    "modelLR = LogisticRegression()\n",
    "modelDT = DecisionTreeClassifier()\n",
    "modelRF = RandomForestClassifier()\n",
    "modelKNN = KNeighborsClassifier()\n",
    "modelNB = GaussianNB()\n",
    "\n",
    "# define the grid search for each model\n",
    "grid_search_lr = GridSearchCV(modelLR, param_grid_lr, cv=5, scoring='accuracy')\n",
    "grid_search_dt = GridSearchCV(modelDT, param_grid_dt, cv=5, scoring='accuracy')\n",
    "grid_search_rf = GridSearchCV(modelRF, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_knn = GridSearchCV(modelKNN, param_grid_knn, cv=5, scoring='accuracy')\n",
    "grid_search_nb = GridSearchCV(modelNB, param_grid_nb, cv=5, scoring='accuracy')\n",
    "\n",
    "# fit the grid search\n",
    "grid_search_lr.fit(X_a, y_a)\n",
    "grid_search_dt.fit(X_a, y_a)\n",
    "grid_search_rf.fit(X_a, y_a)\n",
    "grid_search_knn.fit(X_a, y_a)\n",
    "grid_search_nb.fit(X_a, y_a)\n",
    "\n",
    "\n",
    "print('Best Parameters of Logistic Regression:', grid_search_lr.best_params_)\n",
    "print('Best Cross-Validation Accuracy of Logistic Regression:', grid_search_lr.best_score_)\n",
    "\n",
    "print('Best Parameters of Decision Tree:', grid_search_dt.best_params_)\n",
    "print('Best Cross-Validation Accuracy of Decision Tree:', grid_search_dt.best_score_)\n",
    "\n",
    "print('Best Parameters of Random Forest:', grid_search_rf.best_params_)\n",
    "print('Best Cross-Validation Accuracy of Random Forest:', grid_search_rf.best_score_)\n",
    "\n",
    "print('Best Parameters of KNN:', grid_search_knn.best_params_)\n",
    "print('Best Cross-Validation Accuracy of KNN:', grid_search_knn.best_score_)\n",
    "\n",
    "print('Best Parameters of Naive Bayes:', grid_search_nb.best_params_)\n",
    "print('Best Cross-Validation Accuracy of Naive Bayes:', grid_search_nb.best_score_)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "[ Explain the result ]\n",
    "For each model, I performed grid search for hyperparameter tuning and 5-fold cross-validation, \n",
    "and found that the classification accuracy of the other base models was around 0.5, \n",
    "except for the random forest using ensemble methods.\n",
    "\n",
    "The classification accuracy of the random forest exceeded 0.6.\n",
    "\n",
    "\n",
    "Based on the grid search results with 5-fold cross-validation, the random forest model was selected as the final choice, \n",
    "achieving a classification accuracy of 60.33% with optimized parameters (max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=10). \n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6250\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_a, y_a, \n",
    "    test_size=0.2,\n",
    "    stratify=y_a,\n",
    "    random_state=10\n",
    ")\n",
    "\n",
    "best_rf = RandomForestClassifier(\n",
    "    max_depth=30,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=10,\n",
    "    n_estimators=10,\n",
    "    bootstrap=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix on Test Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "I separately take out the random forest model just mentioned and find the optimal parameters to visualize the confusion matrix.\n",
    "\n",
    "The results are basically consistent.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset B\n",
    "Due to the lack of relevant research, the attempt for Dataset B is only reproduced as part of the proposed architecture of (Thaler et al., 2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 372ms/step - accuracy: 0.4148 - loss: 0.7776 - val_accuracy: 0.7692 - val_loss: 0.7489 - learning_rate: 2.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.4828 - loss: 0.7738 - val_accuracy: 0.6154 - val_loss: 0.7630 - learning_rate: 4.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5925 - loss: 0.7551 - val_accuracy: 0.6154 - val_loss: 0.7698 - learning_rate: 6.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4964 - loss: 0.7655 - val_accuracy: 0.6154 - val_loss: 0.7662 - learning_rate: 8.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5789 - loss: 0.7582 - val_accuracy: 0.6154 - val_loss: 0.7620 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5444 - loss: 0.7738 - val_accuracy: 0.7692 - val_loss: 0.7394 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.6918 - loss: 0.7232 - val_accuracy: 0.6923 - val_loss: 0.7220 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6197 - loss: 0.7383 - val_accuracy: 0.7692 - val_loss: 0.7118 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6845 - loss: 0.7044 - val_accuracy: 0.7692 - val_loss: 0.7096 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7670 - loss: 0.6868 - val_accuracy: 0.6923 - val_loss: 0.7072 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8014 - loss: 0.6321 - val_accuracy: 0.6923 - val_loss: 0.7240 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8046 - loss: 0.6182 - val_accuracy: 0.7692 - val_loss: 0.6991 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7806 - loss: 0.6083 - val_accuracy: 0.6923 - val_loss: 0.6907 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8871 - loss: 0.5368 - val_accuracy: 0.6923 - val_loss: 0.6922 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8663 - loss: 0.5138 - val_accuracy: 0.6923 - val_loss: 0.7446 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8527 - loss: 0.4878 - val_accuracy: 0.6154 - val_loss: 0.7405 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8318 - loss: 0.4982 - val_accuracy: 0.3846 - val_loss: 1.0065 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8631 - loss: 0.4466 - val_accuracy: 0.4615 - val_loss: 1.0529 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9247 - loss: 0.3482 - val_accuracy: 0.6154 - val_loss: 0.9811 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8527 - loss: 0.4369 - val_accuracy: 0.6923 - val_loss: 0.8914 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7294 - loss: 0.6588 - val_accuracy: 0.3077 - val_loss: 1.3178 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9760 - loss: 0.2640 - val_accuracy: 0.3846 - val_loss: 1.6558 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7670 - loss: 0.6213 - val_accuracy: 0.3846 - val_loss: 1.6174 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8767 - loss: 0.4298 - val_accuracy: 0.3077 - val_loss: 1.1692 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8631 - loss: 0.3499 - val_accuracy: 0.5385 - val_loss: 0.9263 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9007 - loss: 0.3532 - val_accuracy: 0.6154 - val_loss: 0.8954 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8975 - loss: 0.3471 - val_accuracy: 0.3846 - val_loss: 0.9984 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9520 - loss: 0.3051 - val_accuracy: 0.3846 - val_loss: 1.3775 - learning_rate: 0.0010\n",
      "Epoch 1/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 379ms/step - accuracy: 0.4724 - loss: 0.7794 - val_accuracy: 0.6154 - val_loss: 0.7632 - learning_rate: 2.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7126 - loss: 0.7549 - val_accuracy: 0.5385 - val_loss: 0.7740 - learning_rate: 4.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5444 - loss: 0.7600 - val_accuracy: 0.3846 - val_loss: 0.7990 - learning_rate: 6.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5653 - loss: 0.7730 - val_accuracy: 0.3846 - val_loss: 0.8066 - learning_rate: 8.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.4860 - loss: 0.7615 - val_accuracy: 0.4615 - val_loss: 0.7961 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.3867 - loss: 0.7882 - val_accuracy: 0.4615 - val_loss: 0.7908 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6165 - loss: 0.7274 - val_accuracy: 0.4615 - val_loss: 0.7974 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.6269 - loss: 0.7328 - val_accuracy: 0.5385 - val_loss: 0.8079 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5789 - loss: 0.7327 - val_accuracy: 0.4615 - val_loss: 0.8267 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6541 - loss: 0.7198 - val_accuracy: 0.4615 - val_loss: 0.8472 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.6029 - loss: 0.6994 - val_accuracy: 0.4615 - val_loss: 0.8666 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6886 - loss: 0.6734 - val_accuracy: 0.5385 - val_loss: 0.8706 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7085 - loss: 0.7011 - val_accuracy: 0.5385 - val_loss: 0.8613 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7910 - loss: 0.6577 - val_accuracy: 0.5385 - val_loss: 0.9086 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7398 - loss: 0.6238 - val_accuracy: 0.5385 - val_loss: 0.9703 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7774 - loss: 0.6304 - val_accuracy: 0.5385 - val_loss: 0.9747 - learning_rate: 0.0010\n",
      "Epoch 1/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 377ms/step - accuracy: 0.5004 - loss: 0.7872 - val_accuracy: 0.3846 - val_loss: 0.8032 - learning_rate: 2.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.4860 - loss: 0.7841 - val_accuracy: 0.3846 - val_loss: 0.7796 - learning_rate: 4.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4932 - loss: 0.7650 - val_accuracy: 0.5385 - val_loss: 0.7634 - learning_rate: 6.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5893 - loss: 0.7574 - val_accuracy: 0.7692 - val_loss: 0.7544 - learning_rate: 8.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5757 - loss: 0.7671 - val_accuracy: 0.7692 - val_loss: 0.7521 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5821 - loss: 0.7542 - val_accuracy: 0.4615 - val_loss: 0.7597 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5308 - loss: 0.7581 - val_accuracy: 0.5385 - val_loss: 0.7653 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5444 - loss: 0.7587 - val_accuracy: 0.4615 - val_loss: 0.7700 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7126 - loss: 0.7311 - val_accuracy: 0.4615 - val_loss: 0.7710 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5653 - loss: 0.7208 - val_accuracy: 0.5385 - val_loss: 0.7489 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7983 - loss: 0.6968 - val_accuracy: 0.6923 - val_loss: 0.7266 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8014 - loss: 0.6529 - val_accuracy: 0.6154 - val_loss: 0.7241 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7910 - loss: 0.6560 - val_accuracy: 0.5385 - val_loss: 0.7324 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7158 - loss: 0.6285 - val_accuracy: 0.6923 - val_loss: 0.7018 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8631 - loss: 0.5913 - val_accuracy: 0.5385 - val_loss: 0.7470 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8014 - loss: 0.5586 - val_accuracy: 0.6154 - val_loss: 0.7221 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9247 - loss: 0.4892 - val_accuracy: 0.6923 - val_loss: 0.7340 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9247 - loss: 0.4413 - val_accuracy: 0.6154 - val_loss: 0.8304 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9007 - loss: 0.4064 - val_accuracy: 0.4615 - val_loss: 1.0495 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9143 - loss: 0.3517 - val_accuracy: 0.3846 - val_loss: 1.1760 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9352 - loss: 0.3515 - val_accuracy: 0.4615 - val_loss: 1.2250 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9279 - loss: 0.2987 - val_accuracy: 0.4615 - val_loss: 1.1829 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9624 - loss: 0.2990 - val_accuracy: 0.5385 - val_loss: 1.1531 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9624 - loss: 0.2503 - val_accuracy: 0.5385 - val_loss: 1.2189 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9624 - loss: 0.2624 - val_accuracy: 0.5385 - val_loss: 1.2204 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8735 - loss: 0.4955 - val_accuracy: 0.6154 - val_loss: 1.1464 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8423 - loss: 0.4729 - val_accuracy: 0.5385 - val_loss: 1.2366 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9624 - loss: 0.2428 - val_accuracy: 0.6154 - val_loss: 1.1774 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9624 - loss: 0.2649 - val_accuracy: 0.6154 - val_loss: 1.2331 - learning_rate: 0.0010\n",
      "Epoch 1/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 378ms/step - accuracy: 0.4213 - loss: 0.7855 - val_accuracy: 0.3077 - val_loss: 0.7741 - learning_rate: 2.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4333 - loss: 0.7781 - val_accuracy: 0.3846 - val_loss: 0.7763 - learning_rate: 4.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6008 - loss: 0.7745 - val_accuracy: 0.4615 - val_loss: 0.7749 - learning_rate: 6.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5817 - loss: 0.7630 - val_accuracy: 0.6154 - val_loss: 0.7572 - learning_rate: 8.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5875 - loss: 0.7474 - val_accuracy: 0.6923 - val_loss: 0.7575 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7004 - loss: 0.7341 - val_accuracy: 0.6923 - val_loss: 0.7674 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.6112 - loss: 0.7333 - val_accuracy: 0.6154 - val_loss: 0.7742 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6350 - loss: 0.7306 - val_accuracy: 0.6154 - val_loss: 0.7638 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6617 - loss: 0.7202 - val_accuracy: 0.6923 - val_loss: 0.7427 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7033 - loss: 0.7039 - val_accuracy: 0.6923 - val_loss: 0.7353 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6854 - loss: 0.7119 - val_accuracy: 0.6923 - val_loss: 0.7562 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7538 - loss: 0.6680 - val_accuracy: 0.6923 - val_loss: 0.7700 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8308 - loss: 0.6480 - val_accuracy: 0.7692 - val_loss: 0.7402 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8517 - loss: 0.6352 - val_accuracy: 0.7692 - val_loss: 0.7203 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8517 - loss: 0.6098 - val_accuracy: 0.6154 - val_loss: 0.7774 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8650 - loss: 0.5841 - val_accuracy: 0.7692 - val_loss: 0.7148 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8621 - loss: 0.5047 - val_accuracy: 0.6923 - val_loss: 0.7262 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8783 - loss: 0.4850 - val_accuracy: 0.6154 - val_loss: 0.7608 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9154 - loss: 0.4477 - val_accuracy: 0.6154 - val_loss: 0.8262 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9154 - loss: 0.4446 - val_accuracy: 0.6154 - val_loss: 0.8571 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9154 - loss: 0.3986 - val_accuracy: 0.5385 - val_loss: 0.9124 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9392 - loss: 0.3340 - val_accuracy: 0.5385 - val_loss: 1.0048 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9629 - loss: 0.3174 - val_accuracy: 0.6923 - val_loss: 0.8650 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8621 - loss: 0.4539 - val_accuracy: 0.5385 - val_loss: 1.0778 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9392 - loss: 0.2977 - val_accuracy: 0.5385 - val_loss: 1.0896 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9496 - loss: 0.3173 - val_accuracy: 0.5385 - val_loss: 1.0591 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9258 - loss: 0.3695 - val_accuracy: 0.6154 - val_loss: 0.9570 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8621 - loss: 0.3879 - val_accuracy: 0.6154 - val_loss: 0.9687 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9125 - loss: 0.3145 - val_accuracy: 0.6154 - val_loss: 0.8585 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9258 - loss: 0.3159 - val_accuracy: 0.6923 - val_loss: 0.9347 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9258 - loss: 0.2763 - val_accuracy: 0.6154 - val_loss: 0.8614 - learning_rate: 0.0010\n",
      "Epoch 1/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 376ms/step - accuracy: 0.4125 - loss: 0.7936 - val_accuracy: 0.4615 - val_loss: 0.7693 - learning_rate: 2.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5608 - loss: 0.7586 - val_accuracy: 0.5385 - val_loss: 0.7672 - learning_rate: 4.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5608 - loss: 0.7682 - val_accuracy: 0.4615 - val_loss: 0.7694 - learning_rate: 6.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5104 - loss: 0.7731 - val_accuracy: 0.3077 - val_loss: 0.7736 - learning_rate: 8.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5817 - loss: 0.7566 - val_accuracy: 0.3846 - val_loss: 0.7807 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5817 - loss: 0.7592 - val_accuracy: 0.4615 - val_loss: 0.7871 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5475 - loss: 0.7555 - val_accuracy: 0.4615 - val_loss: 0.7884 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.5637 - loss: 0.7602 - val_accuracy: 0.3846 - val_loss: 0.7859 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5504 - loss: 0.7522 - val_accuracy: 0.2308 - val_loss: 0.7838 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.6825 - loss: 0.7189 - val_accuracy: 0.2308 - val_loss: 0.7852 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.6692 - loss: 0.7182 - val_accuracy: 0.3846 - val_loss: 0.7928 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7092 - loss: 0.7210 - val_accuracy: 0.3846 - val_loss: 0.7978 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.6646 - loss: 0.6915 - val_accuracy: 0.3846 - val_loss: 0.8132 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7063 - loss: 0.6693 - val_accuracy: 0.4615 - val_loss: 0.8485 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.6513 - loss: 0.6894 - val_accuracy: 0.4615 - val_loss: 0.8734 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7671 - loss: 0.6308 - val_accuracy: 0.3846 - val_loss: 0.9213 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7833 - loss: 0.5783 - val_accuracy: 0.4615 - val_loss: 0.9891 - learning_rate: 0.0010\n",
      "Cross-Validation Scores: [0.5625, 0.4375, 0.625, 0.6000000238418579, 0.6000000238418579]\n",
      "Mean Cross-Validation Accuracy: 0.5650000095367431\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def create_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv1D(32, 3, activation='relu', kernel_regularizer=l2(0.001))(inputs)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Conv1D(64, 3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = LSTM(32)(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 0.001\n",
    "    if epoch < 5:  \n",
    "        return float(initial_lr * ((epoch + 1) / 5))\n",
    "    elif epoch < 30:\n",
    "        return float(initial_lr)\n",
    "    else:\n",
    "        return float(initial_lr * tf.math.exp(0.1 * (30 - epoch)))\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store the cross-validation scores\n",
    "scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_b):\n",
    "    X_train, X_val = X_b[train_index], X_b[val_index]\n",
    "    y_train, y_val = y_b[train_index], y_b[val_index]\n",
    "    \n",
    "    model = create_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "        LearningRateScheduler(lr_schedule),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            min_lr=0.00001\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    scores.append(accuracy)\n",
    "\n",
    "# Cross-validation scores\n",
    "print('Cross-Validation Scores:', scores)\n",
    "print('Mean Cross-Validation Accuracy:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "[ Explain the result ]\n",
    "Without considering parameter tuning, the mean accuracy of 5-fold cross validation is only 0.56, \n",
    "which is worse than the effect of random forest above.\n",
    "\n",
    "I think this is the underfit caused by the insufficient data set.\n",
    "\n",
    "Due to time constraints, I did not try further.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSrJCR_cekPO"
   },
   "source": [
    "# 7 Conclusions\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "[ Conclusion ]\n",
    "After a more reasonable attempt, it was concluded that random forest is best suited for this prediction task. \n",
    "\n",
    "However, due to the small size of the dataset, it may also be due to my failure to fully utilize the dataset, \n",
    "even though the classification accuracy of the best random forest model was only 0.6. \n",
    "\n",
    "I think this issue still deserves further investigation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[ Suggestions for improvements ]\n",
    "I think we can also try data augmentation to increase the richness of the dataset in addition to increasing the data volume. \n",
    "\n",
    "Meanwhile, the selection and combination of input features also need further exploration.\n",
    "\n",
    "Additionally, there is still a lot of exploration to be done with time-series data classification.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 References\n",
    "[1] F. Thaler, S. Faußer, and H. Gewald, \"Using NLP to analyze whether customer statements comply with their inner belief,\" arXiv preprint arXiv:2107.11175v2, 2021. [Online]. Available: https://doi.org/10.48550/arXiv.2107.11175.\n",
    "\n",
    "[2] N. M. Müller, P. Czempin, F. Dieckmann, A. Froghyar, and K. Böttinger, \"Does audio deepfake detection generalize?\" *arXiv preprint arXiv:2203.16263v4*, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2203.16263.\n",
    "\n",
    "[3] H. Rohde and A. Finkelstein, \"An acoustic automated lie detector,\", 2019. [Online]. Available: https://api.semanticscholar.org/CorpusID:204877090.\n",
    "\n",
    "[4] McFee, B., Raffel, C., Liang, D., Ellis, D., McVicar, M., Battenberg, E., & Nieto, O. (2015). librosa: Audio and Music Signal Analysis in Python. In Proceedings of the 14th Python in Science Conference (pp. 18-24).\n",
    "\n",
    "[5] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., ... & Zheng, X. (2016). TensorFlow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467.\n",
    "\n",
    "[6] Zhang, J. (2021). Signal Processing Tool Kit (SPKIT). GitHub repository. https://github.com/Nikeshbajaj/spkit\n",
    "\n",
    "[7] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825-2830."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
